{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as sqlf \n",
    "from pyspark.sql import DataFrameReader, SQLContext\n",
    "#col, lit, udf,sum,avg,max,min,mean,count, udf \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 09:34:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/21 09:34:49 WARN DependencyUtils: Local jar /Users/gonzo/Downloads/postgresql-42.5.0.jar does not exist, skipping.\n",
      "22/10/21 09:34:49 INFO SparkContext: Running Spark version 3.3.0\n",
      "22/10/21 09:34:49 INFO ResourceUtils: ==============================================================\n",
      "22/10/21 09:34:49 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/10/21 09:34:49 INFO ResourceUtils: ==============================================================\n",
      "22/10/21 09:34:49 INFO SparkContext: Submitted application: pyspark-shell\n",
      "22/10/21 09:34:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/10/21 09:34:49 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/10/21 09:34:49 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/10/21 09:34:50 INFO SecurityManager: Changing view acls to: gonzo\n",
      "22/10/21 09:34:50 INFO SecurityManager: Changing modify acls to: gonzo\n",
      "22/10/21 09:34:50 INFO SecurityManager: Changing view acls groups to: \n",
      "22/10/21 09:34:50 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/10/21 09:34:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gonzo); groups with view permissions: Set(); users  with modify permissions: Set(gonzo); groups with modify permissions: Set()\n",
      "22/10/21 09:34:50 INFO Utils: Successfully started service 'sparkDriver' on port 61632.\n",
      "22/10/21 09:34:50 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/10/21 09:34:50 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/10/21 09:34:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/10/21 09:34:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/10/21 09:34:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/10/21 09:34:50 INFO DiskBlockManager: Created local directory at /private/var/folders/4b/jt788yd14v53b1tygytwgxy80000gn/T/blockmgr-a2900fc4-5e46-4075-8776-8d6c13d1237a\n",
      "22/10/21 09:34:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "22/10/21 09:34:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/10/21 09:34:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/10/21 09:34:51 ERROR SparkContext: Failed to add /Users/gonzo/Downloads/postgresql-42.5.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/gonzo/Downloads/postgresql-42.5.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/10/21 09:34:51 INFO Executor: Starting executor ID driver on host gonzalos-mbp.griddynamics.com\n",
      "22/10/21 09:34:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "22/10/21 09:34:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61635.\n",
      "22/10/21 09:34:51 INFO NettyBlockTransferService: Server created on gonzalos-mbp.griddynamics.com:61635\n",
      "22/10/21 09:34:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/10/21 09:34:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, gonzalos-mbp.griddynamics.com, 61635, None)\n",
      "22/10/21 09:34:51 INFO BlockManagerMasterEndpoint: Registering block manager gonzalos-mbp.griddynamics.com:61635 with 366.3 MiB RAM, BlockManagerId(driver, gonzalos-mbp.griddynamics.com, 61635, None)\n",
      "22/10/21 09:34:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, gonzalos-mbp.griddynamics.com, 61635, None)\n",
      "22/10/21 09:34:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, gonzalos-mbp.griddynamics.com, 61635, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CapstoneENV/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.jars', '/Users/gonzo/Downloads/postgresql-42.5.0.jar').getOrCreate()\n",
    "conf = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 11:09:38 INFO SparkUI: Stopped Spark web UI at http://gonzalos-mbp.griddynamics.com:4040\n",
      "22/10/21 11:09:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/10/21 11:09:39 INFO MemoryStore: MemoryStore cleared\n",
      "22/10/21 11:09:39 INFO BlockManager: BlockManager stopped\n",
      "22/10/21 11:09:39 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/10/21 11:09:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/10/21 11:09:39 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import pendulum\n",
    "from datetime import datetime\n",
    "\n",
    "def get_subdirectories(path: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that searchs for sub-directories inside a directory \n",
    "    then returns a list of all the directories names.\n",
    "    \"\"\"\n",
    "    if path.split('/')[7] in ['customer', 'transaction']:\n",
    "        subdirectories = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "        subdirectories = [x.split('/')[8] for x in subdirectories]\n",
    "        csv_directories = [directory.split('_')[1] for directory in subdirectories]\n",
    "        return csv_directories\n",
    "    else:\n",
    "        subdirectories = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "        subdirectories = [x.split('/')[8] for x in subdirectories]\n",
    "        return [datetime.strptime(directory, '%Y-%m-%d').date() for directory in subdirectories] \n",
    "        \n",
    "\n",
    "def get_latest_folder(folders: list) -> str:\n",
    "    \"\"\"\n",
    "    Returns the latest date from the list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(max(folders))\n",
    "    except:\n",
    "        return logging.critical('Couldn\\'t find any sub-directory.')\n",
    "\n",
    "\n",
    "def loadJsonData(json_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function that returns a dataframe from a valid directory that contains jsonlines files.\n",
    "    \"\"\"\n",
    "    latest = get_latest_folder(get_subdirectories(json_path))\n",
    "    if os.path.exists(os.path.dirname(os.path.join(json_path, latest))):\n",
    "        df = spark.read.json(os.path.join(json_path, latest))\n",
    "        return df\n",
    "    else:\n",
    "        return logging.critical('Path to jsonl files doesn\\'t exist')\n",
    "\n",
    "\n",
    "def loadParquetData(parquet_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function that returns a dataframe from a valid directory that contains parquet files.\n",
    "    \"\"\"\n",
    "    latest = get_latest_folder(get_subdirectories(parquet_path))\n",
    "    if os.path.exists(os.path.dirname(parquet_path)):\n",
    "        df = spark.read.option('recursiveFileLookup', 'true').option('header', 'true').parquet(parquet_path)\n",
    "        return df\n",
    "    else:\n",
    "        return logging.critical('Path to parquet file deosn\\'t exist')\n",
    "\n",
    "\n",
    "# def loadCSVData(csv_path: str) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Function that returns a dataframe from a valid directory that contains csv files.\n",
    "#     \"\"\"\n",
    "#     latest = get_latest_folder(get_subdirectories(csv_path))\n",
    "#     if os.path.exists(os.path.dirname(csv_path)):\n",
    "#         df = spark.read.option('recursiveFileLookup', 'true').option('header', 'true').csv(csv_path)\n",
    "#         return df\n",
    "#     else:\n",
    "#         return logging.critical('Path to csv file doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 09:34:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/10/21 09:34:52 INFO SharedState: Warehouse path is 'file:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/spark-warehouse'.\n",
      "22/10/21 09:34:54 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.\n",
      "22/10/21 09:34:54 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "22/10/21 09:34:57 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/10/21 09:34:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/10/21 09:34:57 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/10/21 09:34:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 350.2 KiB, free 366.0 MiB)\n",
      "22/10/21 09:34:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 365.9 MiB)\n",
      "22/10/21 09:34:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on gonzalos-mbp.griddynamics.com:61635 (size: 34.0 KiB, free: 366.3 MiB)\n",
      "22/10/21 09:34:58 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0\n",
      "22/10/21 09:34:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/10/21 09:34:58 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0\n",
      "22/10/21 09:34:58 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/21 09:34:58 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)\n",
      "22/10/21 09:34:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/21 09:34:58 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/21 09:34:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/21 09:34:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.7 KiB, free 365.9 MiB)\n",
      "22/10/21 09:34:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 365.9 MiB)\n",
      "22/10/21 09:34:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on gonzalos-mbp.griddynamics.com:61635 (size: 7.1 KiB, free: 366.3 MiB)\n",
      "22/10/21 09:34:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/21 09:34:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/21 09:34:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/10/21 09:34:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (gonzalos-mbp.griddynamics.com, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\n",
      "22/10/21 09:34:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/10/21 09:34:59 INFO FileScanRDD: Reading File path: file:///Users/gonzo/Desktop/capstone_project/airflow/dags/json_storage/2022-10-19/jsonl_2022-10-19, range: 0-13831, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 09:34:59 INFO CodeGenerator: Code generated in 330.396496 ms\n",
      "22/10/21 09:34:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2144 bytes result sent to driver\n",
      "22/10/21 09:34:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 869 ms on gonzalos-mbp.griddynamics.com (executor driver) (1/1)\n",
      "22/10/21 09:34:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/10/21 09:34:59 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.019 s\n",
      "22/10/21 09:34:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/21 09:34:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/10/21 09:34:59 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.080717 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 09:34:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/10/21 09:34:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/10/21 09:34:59 INFO FileSourceStrategy: Output Data Schema: struct<amount: string, customer_first_name: string, customer_last_name: string, id: bigint, ts: string ... 1 more field>\n",
      "22/10/21 09:35:00 INFO CodeGenerator: Code generated in 30.118872 ms\n",
      "22/10/21 09:35:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 350.1 KiB, free 365.6 MiB)\n",
      "22/10/21 09:35:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.5 MiB)\n",
      "22/10/21 09:35:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on gonzalos-mbp.griddynamics.com:61635 (size: 34.1 KiB, free: 366.2 MiB)\n",
      "22/10/21 09:35:00 INFO SparkContext: Created broadcast 2 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "22/10/21 09:35:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/10/21 09:35:00 INFO DAGScheduler: Registering RDD 8 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "22/10/21 09:35:00 INFO DAGScheduler: Got map stage job 1 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/21 09:35:00 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/10/21 09:35:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/21 09:35:00 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/21 09:35:00 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/21 09:35:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.2 KiB, free 365.5 MiB)\n",
      "22/10/21 09:35:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 365.5 MiB)\n",
      "22/10/21 09:35:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on gonzalos-mbp.griddynamics.com:61635 (size: 8.2 KiB, free: 366.2 MiB)\n",
      "22/10/21 09:35:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/21 09:35:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/21 09:35:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/10/21 09:35:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (gonzalos-mbp.griddynamics.com, executor driver, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\n",
      "22/10/21 09:35:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/10/21 09:35:00 INFO FileScanRDD: Reading File path: file:///Users/gonzo/Desktop/capstone_project/airflow/dags/json_storage/2022-10-19/jsonl_2022-10-19, range: 0-13831, partition values: [empty row]\n",
      "22/10/21 09:35:00 INFO CodeGenerator: Code generated in 37.275787 ms\n",
      "22/10/21 09:35:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1844 bytes result sent to driver\n",
      "22/10/21 09:35:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 286 ms on gonzalos-mbp.griddynamics.com (executor driver) (1/1)\n",
      "22/10/21 09:35:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/10/21 09:35:00 INFO DAGScheduler: ShuffleMapStage 1 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.349 s\n",
      "22/10/21 09:35:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/10/21 09:35:00 INFO DAGScheduler: running: Set()\n",
      "22/10/21 09:35:00 INFO DAGScheduler: waiting: Set()\n",
      "22/10/21 09:35:00 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting jsonlines data\n",
    "jsonDF = loadJsonData(json_path='/Users/gonzo/Desktop/capstone_project/airflow/dags/json_storage/')\n",
    "jsonDF = jsonDF.select('id', 'ts', 'customer_first_name', 'customer_last_name', 'amount', 'type')\n",
    "jsonDF = jsonDF.repartition(2)\n",
    "jsonDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loadParquetData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gonzo/Desktop/capstone_project/pyspark/jupiter notebooks/test_data_loading.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/test_data_loading.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39m# Ingesting parquet data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/test_data_loading.ipynb#ch0000004?line=1'>2</a>\u001b[0m parquetDF \u001b[39m=\u001b[39m loadParquetData(parquet_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/Users/gonzo/Desktop/capstone_project/airflow/dags/parquet_storage/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/test_data_loading.ipynb#ch0000004?line=2'>3</a>\u001b[0m parquetDF \u001b[39m=\u001b[39m parquetDF\u001b[39m.\u001b[39mrepartition(\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/test_data_loading.ipynb#ch0000004?line=3'>4</a>\u001b[0m parquetDF\u001b[39m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loadParquetData' is not defined"
     ]
    }
   ],
   "source": [
    "# Ingesting parquet data\n",
    "parquetDF = loadParquetData(parquet_path='/Users/gonzo/Desktop/capstone_project/airflow/dags/parquet_storage/')\n",
    "parquetDF = parquetDF.repartition(2)\n",
    "parquetDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection to PostgreSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o105.load.\n: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:331)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:247)\n\tat org.postgresql.Driver.makeConnection(Driver.java:434)\n\tat org.postgresql.Driver.connect(Driver.java:291)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:122)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:118)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:242)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.SocketException: Network is unreachable (connect failed)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:476)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:218)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:200)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:394)\n\tat java.net.Socket.connect(Socket.java:606)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:241)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/gonzo/Desktop/capstone_project/pyspark/jupiter notebooks/test_data_loading.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/test_data_loading.ipynb#ch0000098?line=0'>1</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mjdbc:postgresql://192.168.16.4:5432/capstone\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/test_data_loading.ipynb#ch0000098?line=1'>2</a>\u001b[0m properties \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpassword\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gonzo/Desktop/capstone_project/pyspark/jupiter%20notebooks/test_data_loading.ipynb#ch0000098?line=2'>3</a>\u001b[0m postgresql_df \u001b[39m=\u001b[39m sqlContext\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mjdbc\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moptions(url\u001b[39m=\u001b[39;49murl, driver\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39morg.postgresql.Driver\u001b[39;49m\u001b[39m'\u001b[39;49m, dbtable\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcustomer\u001b[39;49m\u001b[39m'\u001b[39;49m, properties\u001b[39m=\u001b[39;49mproperties)\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CapstoneENV/lib/python3.10/site-packages/pyspark/sql/readwriter.py:184\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CapstoneENV/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CapstoneENV/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CapstoneENV/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o105.load.\n: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:331)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:247)\n\tat org.postgresql.Driver.makeConnection(Driver.java:434)\n\tat org.postgresql.Driver.connect(Driver.java:291)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:122)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:118)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:242)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.SocketException: Network is unreachable (connect failed)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:476)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:218)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:200)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:394)\n\tat java.net.Socket.connect(Socket.java:606)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:241)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "url = 'jdbc:postgresql://192.168.16.4:5432/capstone'\n",
    "properties = {'user': 'root', 'password': 'root'}\n",
    "postgresql_df = sqlContext.read.format('jdbc').options(url=url, driver='org.postgresql.Driver', dbtable='customer', properties=properties).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+------------+-------+\n",
      "| id|first_name|last_name|phone_number|address|\n",
      "+---+----------+---------+------------+-------+\n",
      "+---+----------+---------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postgresql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union between json and parquet DF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetDF = parquetDF.withColumnRenamed('First_name', 'customer_first_name')\n",
    "parquetDF = parquetDF.withColumnRenamed('Last_name', 'customer_last_name')\n",
    "parquetDF = parquetDF.withColumnRenamed('Amount', 'amount')\n",
    "jsonDF = jsonDF.withColumnRenamed('ts', 'timestamp')\n",
    "jsonDF = jsonDF.withColumnRenamed('Store_id', 'store_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet & Json\n",
    "json_parquetDF = jsonDF.join(parquetDF, ['id', 'customer_first_name', 'customer_last_name', 'amount', 'timestamp'], \"fullouter\")\n",
    "json_parquetDF = json_parquetDF.select('id', 'type', 'store_id', 'amount', 'customer_first_name', 'customer_last_name', 'timestamp')\n",
    "json_parquetDF = json_parquetDF.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+------------+-----------------+-------------------+\n",
      "| id|customer_id|    amount| first_name| last_name|phone_number|          address|     transaction_ts|\n",
      "+---+-----------+----------+-----------+----------+------------+-----------------+-------------------+\n",
      "|778|         39| $8,278.52|    Cynthia|   Johnson|  3253956605|      Veronicaton|2022-10-03 15:09:37|\n",
      "| 33|        191| $2,917.29|       John|     Burns|  8671875822|Port Benjaminfurt|2022-10-03 05:58:39|\n",
      "|482|        106| $7,042.53|      Carol| Rodriguez|  9808399017|      Veronicaton|2022-10-03 03:48:50|\n",
      "|609|        759|    $41.25|    Cynthia| Hernandez|  1825888381|       Robertland|2022-10-03 22:08:51|\n",
      "|459|        358|   $709.30|       Leah|      Ward|  2449557321|      Lake Karina|2022-10-03 02:52:32|\n",
      "|414|         52|    $50.11|      David|      Barr|  2388757413|      Veronicaton|2022-10-04 16:07:52|\n",
      "|556|        808|     $6.85|      Kelly|     Smith|  3880545614|        Smithberg|2022-10-04 11:57:27|\n",
      "|199|        666|$12,611.06|       Lynn|   Swanson|  1308999765|        Erikville|2022-10-03 20:20:26|\n",
      "|633|        691|    $82.76|    Rebecca|     White|  8816293027|         Clayview|2022-10-03 14:36:35|\n",
      "|299|        387| $3,098.35|      Peter|    Fisher|  4775028457|Port Benjaminfurt|2022-10-04 04:36:48|\n",
      "|524|        601|     $4.16|     Robert|   Bullock|  6224540673|         Clayview|2022-10-03 12:21:37|\n",
      "|761|        652|   $425.28|Christopher|   Stevens|  9911692402|New Marthaborough|2022-10-04 20:20:26|\n",
      "|249|        796|$23,476.67|    Tiffany|  Garrison|  2366958709|      Melissafurt|2022-10-03 19:17:45|\n",
      "|593|        846| $8,504.85|      David|  Thompson|  6354258810|       Averymouth|2022-10-04 16:19:48|\n",
      "|222|          2|   $382.70|      Jesse|  Williams|  2446792417|     Brownchester|2022-10-02 18:46:05|\n",
      "|851|        409|    $41.00|Christopher|    Harvey|  8820598051|       Averymouth|2022-10-04 12:17:51|\n",
      "| 41|        840|    $81.96|   Veronica|  Williams|  4852270998|         Lanefurt|2022-10-02 19:11:14|\n",
      "|559|         89|$20,344.56|    Douglas|  Phillips|  0812632263|         Lanefurt|2022-10-04 15:47:47|\n",
      "|544|        594|$65,182.95|    Melissa|  Thompson|  5220792018|Port Benjaminfurt|2022-10-02 14:20:34|\n",
      "|237|        362| $6,350.88|   Mckenzie|      Dean|  0832472650|        Smithberg|2022-10-03 22:50:00|\n",
      "|308|        295|     $9.41|  Alexander|    Clarke|  1182263360|     Edwardsburgh|2022-10-03 19:56:15|\n",
      "|694|        221|   $613.15|      Megan|     Smith|  7740943003|     Edwardsburgh|2022-10-05 05:45:14|\n",
      "|679|        423|$22,232.93|      Julie|   Gilmore|  2993695570|      Melissafurt|2022-10-04 20:55:32|\n",
      "|129|        837|    $27.51|    Tiffany|   Farrell|  0174130827|New Marthaborough|2022-10-03 10:40:03|\n",
      "| 12|        705|    $37.96|   Patricia|  Williams|  1624390971|Port Benjaminfurt|2022-10-03 02:41:43|\n",
      "|500|        873|$10,513.60|     Tamara|     Smith|  0923886379|      Veronicaton|2022-10-03 11:52:49|\n",
      "| 43|        405|    $91.78|    Zachary|    Ramsey|  6457314142|        Hicksview|2022-10-05 05:53:04|\n",
      "|617|        331|    $11.43|      Tammy|     Huang|  0871774292|     Brownchester|2022-10-04 06:41:17|\n",
      "|240|        571|   $125.38|   Victoria|   Hendrix|  6381835167|      Lake Karina|2022-10-04 08:19:08|\n",
      "|444|        361|     $8.35|     Amanda|    Nelson|  6123614837|    Michelleburgh|2022-10-02 16:33:55|\n",
      "|688|        488|$35,858.27|    William| Rasmussen|  7734863626|New Marthaborough|2022-10-05 04:40:34|\n",
      "|424|        891|   $734.20|     Robert|   Brennan|  5366474143|     West Nichole|2022-10-03 12:45:04|\n",
      "|325|        851|    $71.47|    Zachary|     Perez|  8603441510|       Grahamstad|2022-10-04 18:12:23|\n",
      "|705|         87|    $61.52|      Scott|   Vazquez|  7187411263|      Melissafurt|2022-10-03 11:58:10|\n",
      "|658|        110|   $186.95|      Kevin|     Scott|  3209615463|       Averymouth|2022-10-02 13:43:54|\n",
      "|803|        211|   $414.29|       Mary|   Mcmahon|  8765744533|     Brownchester|2022-10-05 05:16:16|\n",
      "| 18|        363|   $654.36|     Denise|  Anderson|  5718166161|      Melissafurt|2022-10-03 12:56:18|\n",
      "|756|        125|$72,918.43|    Heather|Montgomery|  7380781141|Port Benjaminfurt|2022-10-05 06:41:24|\n",
      "|285|        607|     $6.33|       Adam| Armstrong|  7507613719|        Erikville|2022-10-04 03:13:54|\n",
      "|830|        210|    $73.33|      Diane|     Henry|  2108985424|    Port Loriview|2022-10-04 15:19:42|\n",
      "|826|        228| $3,486.91|      Tammy|     Simon|  9490955624|        Erikville|2022-10-03 06:28:18|\n",
      "|691|        519|   $417.90|      Jason| Rodriguez|  3327301660|     Brownchester|2022-10-03 21:06:23|\n",
      "|741|        576| $8,222.15|    Matthew|   Johnson|  0861318748|        Erikville|2022-10-02 12:58:01|\n",
      "|734|        291|$53,224.48|     Nicole|   Carroll|  0736462580|    Michelleburgh|2022-10-04 22:42:00|\n",
      "|341|        442|   $188.14|       Carl|  Stephens|  3822009809|      Lake Karina|2022-10-03 22:45:02|\n",
      "|242|        170| $8,151.78|   Kimberly|   Cameron|  6044982568|         Clayview|2022-10-02 14:11:59|\n",
      "|626|        680|$20,339.45|      Jamie|     Smith|  3296690045|      Veronicaton|2022-10-03 09:06:42|\n",
      "|539|        480|$52,058.45|     Ashley|  Williams|  2481339010|       Grahamstad|2022-10-02 16:23:42|\n",
      "|666|        617|   $115.50|       Kara|    Wagner|  1169674575|        Erikville|2022-10-05 08:56:02|\n",
      "|555|        268|$48,620.80|       Dawn|    Butler|  0389827908|     Brownchester|2022-10-05 05:34:39|\n",
      "|280|        509|$30,407.53|      Haley|   Hawkins|  8642515731|      Melissafurt|2022-10-03 11:34:32|\n",
      "|276|         99|$13,469.52|  Christine|   Jenkins|  8127765681|Port Benjaminfurt|2022-10-04 20:39:11|\n",
      "|460|         52| $1,517.44|     Steven|     James|  6944540810|        Hicksview|2022-10-03 03:00:21|\n",
      "|170|        689| $4,249.06|     Arthur|    Hester|  9696686678|     West Nichole|2022-10-03 08:34:01|\n",
      "|230|        135|$56,969.13|     Rodney|    Hansen|  8320254682|     West Nichole|2022-10-04 02:20:57|\n",
      "|881|        367|   $375.75|     Tamara|  Gonzales|  5134088885|      Veronicaton|2022-10-05 07:22:57|\n",
      "|671|        753|   $566.06|    Jessica|     Scott|  4499703128|        Smithberg|2022-10-05 11:43:50|\n",
      "|634|        753| $8,806.84|     Sharon|    Garcia|  2640279537|Port Benjaminfurt|2022-10-05 11:36:34|\n",
      "|288|        756| $2,626.56|      Megan|     Weber|  6331206019|  Lake Jamesville|2022-10-04 00:58:10|\n",
      "|550|        537|     $5.56|    Christy|   Webster|  1708396924|Port Benjaminfurt|2022-10-05 10:47:53|\n",
      "|447|        383|$41,071.09|   Reginald| Mcconnell|  2222850799|      Lake Karina|2022-10-04 21:50:34|\n",
      "|816|        683| $8,078.37|      Julie|    Dorsey|  6583481878|       Averymouth|2022-10-04 22:16:42|\n",
      "|854|        404|    $22.96|   Brittany|     Allen|  5632846459|         Allenton|2022-10-04 08:57:35|\n",
      "|678|        740|$99,439.55|       John|     Davis|  5598504008|      Lake Karina|2022-10-04 14:40:36|\n",
      "|383|        389|    $82.44|      Donna|    Rivera|  1444524704|    Michelleburgh|2022-10-04 22:50:10|\n",
      "|528|        881|   $207.13|   Cristian|    Romero|  5197726562|       Averymouth|2022-10-05 00:11:31|\n",
      "|315|        753|$55,575.70| Jacqueline|   Bennett|  5372596139|     West Nichole|2022-10-03 09:12:15|\n",
      "|358|        656|     $6.60|       Adam|    Benson|  5025744946|        Smithberg|2022-10-02 23:59:20|\n",
      "|508|         64| $9,997.73|      Kevin|      Ward|  3866130003|         Allenton|2022-10-05 04:13:04|\n",
      "|781|        848|    $20.63|      Jason|   Jenkins|  3604057231|        Erikville|2022-10-05 11:22:41|\n",
      "|398|        678|     $0.67|     Samuel|     Lewis|  0638466687|        Smithberg|2022-10-04 10:25:06|\n",
      "|337|        286| $8,535.49|    Felicia|     Jones|  1839349418|         Lanefurt|2022-10-04 07:12:48|\n",
      "| 59|        671| $7,849.37|     Jordan|    Fisher|  6219321808|         Allenton|2022-10-04 01:40:54|\n",
      "|788|        736|$19,425.80|     Marcus|   Everett|  7822771630|       Robertland|2022-10-03 13:06:28|\n",
      "|379|        229|$49,262.72|       Eric|   Stanley|  5302630562|       Robertland|2022-10-02 19:10:03|\n",
      "|534|         53| $2,796.26|    Bradley|      Cook|  7111430381|       Averymouth|2022-10-02 15:00:16|\n",
      "|218|        329|   $381.18|   Kimberly|     Baker|  5596293609|         Clayview|2022-10-04 16:49:12|\n",
      "| 58|        462|   $115.29|       Ryan|    Garcia|  3285654271|         Lanefurt|2022-10-02 20:56:17|\n",
      "|875|        680|    $79.78|     Manuel|     Owens|  4537434977|      Lake Karina|2022-10-04 21:20:57|\n",
      "| 17|        530|$65,981.69|  Stephanie|    Chavez|  9952997214|    Michelleburgh|2022-10-02 15:00:14|\n",
      "| 83|         13| $4,171.11|    Gregory|  Mcmillan|  6535423571|         Clayview|2022-10-05 00:28:50|\n",
      "| 54|        423|    $79.18|    Michael|   Johnson|  8947261122|    Port Loriview|2022-10-03 00:17:35|\n",
      "|893|        699|   $216.84|     Dustin|      Luna|  2815708255|     Brownchester|2022-10-03 17:58:18|\n",
      "|890|        472|     $9.38|      Jared|Fitzgerald|  7449373848|      Veronicaton|2022-10-04 00:11:07|\n",
      "|849|        305| $2,595.04|    Yolanda|    Hanson|  2292817824|        Hicksview|2022-10-03 00:04:06|\n",
      "|611|        196|   $583.06|       Joel|    Murray|  0407299545|  Lake Jamesville|2022-10-05 07:19:21|\n",
      "|613|        288|   $988.37|      Amber|     Baker|  1010874540|     West Nichole|2022-10-03 21:42:16|\n",
      "|226|        310|   $107.21|     Edward|  Galloway|  9966544188|      Melissafurt|2022-10-03 14:23:16|\n",
      "|866|        371| $3,146.52|      Debra|   Ramirez|  6760142880|         Clayview|2022-10-04 05:43:48|\n",
      "|628|        709|$18,344.42|    Brandon|     Logan|  4074657274|      Lake Karina|2022-10-03 07:12:31|\n",
      "|343|        893|     $9.29|      Susan|     Jones|  6629756527|     West Nichole|2022-10-05 08:36:28|\n",
      "|219|        308|     $8.29|     Justin|    Waters|  2556250708|     Brownchester|2022-10-04 20:44:06|\n",
      "|836|        316|$59,665.01|    Bethany|     Brock|  5737632002|        Smithberg|2022-10-03 01:38:03|\n",
      "|668|        876|     $0.63|     Angela|    Becker|  0645754275|       Grahamstad|2022-10-03 05:48:02|\n",
      "|623|        641|     $6.13|       Adam|     Mejia|  5345726733|        Erikville|2022-10-03 15:05:04|\n",
      "|445|        813| $7,341.87|       Mary|    Fields|  2820740098|       Robertland|2022-10-04 19:23:19|\n",
      "|419|        541|   $757.13|Christopher|    Murray|  9982797324|         Lanefurt|2022-10-03 06:07:04|\n",
      "|526|         35|     $1.21|       Jose|     Perry|  0189161517|        Smithberg|2022-10-04 16:10:51|\n",
      "|701|        408|$27,460.40|       Todd|    Golden|  4316158441|    Port Loriview|2022-10-05 12:18:57|\n",
      "|598|        396|$72,402.01|    Michael|     Ortiz|  7164476916|Port Benjaminfurt|2022-10-03 12:02:48|\n",
      "+---+-----------+----------+-----------+----------+------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Both dataframes from postgresql\n",
    "postgresqlDF = customerDF.join(transactionDF, ['id'], \"inner\")\n",
    "postgresqlDF = postgresqlDF.select('id', 'customer_id', 'amount', 'first_name', 'last_name', 'phone_number', 'address', 'transaction_ts')\n",
    "postgresqlDF.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-------------------+------------------+------------+-----------------+----------+\n",
      "| id|customer_id|    amount|customer_first_name|customer_last_name|phone_number|          address| timestamp|\n",
      "+---+-----------+----------+-------------------+------------------+------------+-----------------+----------+\n",
      "|778|        778| $8,278.52|            Cynthia|           Johnson|  3253956605|      Veronicaton|2022-10-03|\n",
      "| 33|         33| $2,917.29|               John|             Burns|  8671875822|Port Benjaminfurt|2022-10-03|\n",
      "|482|        482| $7,042.53|              Carol|         Rodriguez|  9808399017|      Veronicaton|2022-10-03|\n",
      "|609|        609|    $41.25|            Cynthia|         Hernandez|  1825888381|       Robertland|2022-10-03|\n",
      "|459|        459|   $709.30|               Leah|              Ward|  2449557321|      Lake Karina|2022-10-03|\n",
      "|414|        414|    $50.11|              David|              Barr|  2388757413|      Veronicaton|2022-10-04|\n",
      "|556|        556|     $6.85|              Kelly|             Smith|  3880545614|        Smithberg|2022-10-04|\n",
      "|199|        199|$12,611.06|               Lynn|           Swanson|  1308999765|        Erikville|2022-10-03|\n",
      "|633|        633|    $82.76|            Rebecca|             White|  8816293027|         Clayview|2022-10-03|\n",
      "|299|        299| $3,098.35|              Peter|            Fisher|  4775028457|Port Benjaminfurt|2022-10-04|\n",
      "|524|        524|     $4.16|             Robert|           Bullock|  6224540673|         Clayview|2022-10-03|\n",
      "|761|        761|   $425.28|        Christopher|           Stevens|  9911692402|New Marthaborough|2022-10-04|\n",
      "|249|        249|$23,476.67|            Tiffany|          Garrison|  2366958709|      Melissafurt|2022-10-03|\n",
      "|593|        593| $8,504.85|              David|          Thompson|  6354258810|       Averymouth|2022-10-04|\n",
      "|222|        222|   $382.70|              Jesse|          Williams|  2446792417|     Brownchester|2022-10-02|\n",
      "|851|        851|    $41.00|        Christopher|            Harvey|  8820598051|       Averymouth|2022-10-04|\n",
      "| 41|         41|    $81.96|           Veronica|          Williams|  4852270998|         Lanefurt|2022-10-02|\n",
      "|559|        559|$20,344.56|            Douglas|          Phillips|  0812632263|         Lanefurt|2022-10-04|\n",
      "|544|        544|$65,182.95|            Melissa|          Thompson|  5220792018|Port Benjaminfurt|2022-10-02|\n",
      "|237|        237| $6,350.88|           Mckenzie|              Dean|  0832472650|        Smithberg|2022-10-03|\n",
      "+---+-----------+----------+-------------------+------------------+------------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postgresqlDF = postgresqlDF.withColumnRenamed('first_name', 'customer_first_name')\n",
    "postgresqlDF = postgresqlDF.withColumnRenamed('last_name', 'customer_last_name')\n",
    "postgresqlDF = postgresqlDF.withColumnRenamed('transaction_ts', 'timestamp')\n",
    "\"\"\"\n",
    "Change id str -> long\n",
    "customer_id str -> long\n",
    "timestamp remove ' ' \n",
    "\"\"\"\n",
    "def transform_timestampPSQL(x) -> str:\n",
    "    return x.split(' ')[0]\n",
    "\n",
    "timestampPSQLUDF = sqlf.udf(lambda x : transform_timestampPSQL(x), StringType())\n",
    "\n",
    "postgresqlDF = postgresqlDF.withColumn('id', postgresqlDF['id'].cast(LongType()))\n",
    "postgresqlDF = postgresqlDF.withColumn('customer_id', postgresqlDF['id'].cast(LongType()))\n",
    "postgresqlDF = postgresqlDF.withColumn('timestamp', timestampPSQLUDF(postgresqlDF.timestamp))\n",
    "postgresqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unified_model = json_parquetDF.join(postgresqlDF, ['id', 'amount', 'customer_first_name', 'customer_last_name', 'timestamp'], 'fullouter')\n",
    "unified_model = unified_model.select('id', 'customer_id', 'store_id', 'type', 'amount', 'customer_first_name', 'customer_last_name', 'phone_number', 'address', 'timestamp')\n",
    "unified_model.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unified_model.coalesce(1)\n",
    "# unified_model = unified_model.repartition(2)\n",
    "# unified_model.write.csv('/Users/gonzo/Desktop/capstone_project/data_storage/storage/test', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transform amount str -> float\n",
    "\"\"\"\n",
    "\n",
    "def transform_amount(x) -> list:\n",
    "    return float(x[1:].replace(',', ''))\n",
    "\n",
    "\n",
    "def transform_timestamp(x) -> str:\n",
    "    return x.split('T')[0]\n",
    "\n",
    "transformUDF = sqlf.udf(lambda x : transform_amount(x), FloatType())\n",
    "timestampUDF = sqlf.udf(lambda x : transform_timestamp(x), StringType())\n",
    "\n",
    "dumpDF = unified_model.withColumn('float_amount', transformUDF(unified_model.amount))\n",
    "dumpDF = dumpDF.withColumn('timestamp', timestampUDF(dumpDF.timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_model = dumpDF.select('id', 'customer_id', 'store_id', 'type', 'float_amount', 'customer_first_name', 'customer_last_name', 'phone_number', 'address', 'timestamp')\n",
    "unified_model = unified_model.withColumnRenamed('float_amount', 'amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----+--------+-------------------+------------------+------------+-----------------+----------+\n",
      "| id|customer_id|store_id|type|  amount|customer_first_name|customer_last_name|phone_number|          address| timestamp|\n",
      "+---+-----------+--------+----+--------+-------------------+------------------+------------+-----------------+----------+\n",
      "|  2|       null|    null|   1|    4.18|             Brenda|           Sanchez|        null|             null|2022-10-02|\n",
      "|  4|       null|    null|   0|23199.16|               Dawn|              Hill|        null|             null|2022-10-03|\n",
      "|  6|       null|    null|   0|84143.57|            Jeffrey|          Gonzales|        null|             null|2022-10-04|\n",
      "|  7|       null|      17|null|  711.35|             Thomas|            Oliver|        null|             null|2022-10-05|\n",
      "| 10|       null|      20|null|    67.4|            William|             Reyes|        null|             null|2022-10-04|\n",
      "| 11|       null|       2|null| 4516.14|             Jeremy|             Moody|        null|             null|2022-10-03|\n",
      "| 12|         12|    null|null|   37.96|           Patricia|          Williams|  1624390971|Port Benjaminfurt|2022-10-03|\n",
      "| 17|         17|    null|null|65981.69|          Stephanie|            Chavez|  9952997214|    Michelleburgh|2022-10-02|\n",
      "| 18|         18|    null|null|  654.36|             Denise|          Anderson|  5718166161|      Melissafurt|2022-10-03|\n",
      "| 19|       null|    null|   0|    7.75|              Sarah|             Craig|        null|             null|2022-10-04|\n",
      "| 23|       null|      18|null|   24.07|             Brandi|             Ortiz|        null|             null|2022-10-03|\n",
      "| 28|       null|      13|null|   73.06|            Anthony|            Harper|        null|             null|2022-10-03|\n",
      "| 33|         33|    null|null| 2917.29|               John|             Burns|  8671875822|Port Benjaminfurt|2022-10-03|\n",
      "| 41|         41|    null|null|   81.96|           Veronica|          Williams|  4852270998|         Lanefurt|2022-10-02|\n",
      "| 43|         43|    null|null|   91.78|            Zachary|            Ramsey|  6457314142|        Hicksview|2022-10-05|\n",
      "| 45|       null|      17|null|  654.54|             Joshua|           Johnson|        null|             null|2022-10-02|\n",
      "| 46|       null|    null|   0|  574.71|              Sarah|             Jones|        null|             null|2022-10-04|\n",
      "| 47|       null|    null|   0|  483.24|              Peter|              Mack|        null|             null|2022-10-04|\n",
      "| 54|         54|    null|null|   79.18|            Michael|           Johnson|  8947261122|    Port Loriview|2022-10-03|\n",
      "| 57|       null|       9|null| 9925.79|            Matthew|             Scott|        null|             null|2022-10-04|\n",
      "+---+-----------+--------+----+--------+-------------------+------------------+------------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unified_model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count and sum amount transactions for each type for day - online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+------------------+\n",
      "|type| timestamp|total_transactions|             total|\n",
      "+----+----------+------------------+------------------+\n",
      "|   1|2022-10-02|                10|171386.61967229843|\n",
      "|   1|2022-10-03|                10| 69591.97856712341|\n",
      "|   1|2022-10-04|                17| 80872.64190080762|\n",
      "|   1|2022-10-05|                 9|185163.73170089722|\n",
      "+----+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online = unified_model.select('type', 'timestamp', 'amount')\n",
    "online = online.filter(online.type == 1).groupBy(online.type, online.timestamp).agg(sqlf.count('*').alias('total_transactions'), sqlf.sum('amount').alias('total'))\n",
    "online.sort('timestamp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count and sum amount transactions for each type for day - offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+------------------+\n",
      "|type| timestamp|total_transactions|             total|\n",
      "+----+----------+------------------+------------------+\n",
      "|   0|2022-10-02|                 6|1485.7499787807465|\n",
      "|   0|2022-10-03|                16|179331.11799812317|\n",
      "|   0|2022-10-04|                25|231126.67954114825|\n",
      "|   0|2022-10-05|                 7| 4441.390130549669|\n",
      "+----+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offline = unified_model.select('type', 'timestamp', 'amount')\n",
    "offline = offline.filter(offline.type == 0).groupBy(offline.type, offline.timestamp).agg(sqlf.count('*').alias('total_transactions'), sqlf.sum('amount').alias('total'))\n",
    "offline.sort('timestamp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count and sum transaction for each store for day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+------------------+\n",
      "|store_id| timestamp|count|             total|\n",
      "+--------+----------+-----+------------------+\n",
      "|    null|2022-10-02|   29| 425310.2145335674|\n",
      "|      18|2022-10-02|    2| 782.8299789428711|\n",
      "|      12|2022-10-02|    1|  98.4000015258789|\n",
      "|       8|2022-10-02|    1|  68.2699966430664|\n",
      "|       3|2022-10-02|    1|     66788.5703125|\n",
      "|       5|2022-10-02|    1|  1369.56005859375|\n",
      "|      17|2022-10-02|    3| 718.2699794769287|\n",
      "|      10|2022-10-02|    1| 435.4100036621094|\n",
      "|      20|2022-10-02|    1| 58.08000183105469|\n",
      "|       7|2022-10-02|    1| 1110.449951171875|\n",
      "|      14|2022-10-02|    2| 831.7099990844727|\n",
      "|       6|2022-10-02|    2| 9549.819726467133|\n",
      "|    null|2022-10-03|   63| 613245.9728010893|\n",
      "|      18|2022-10-03|    3| 7946.419731140137|\n",
      "|       9|2022-10-03|    2| 10383.47998046875|\n",
      "|      12|2022-10-03|    4| 90181.04906272888|\n",
      "|       8|2022-10-03|    3|183019.29092407227|\n",
      "|       1|2022-10-03|    4| 84645.54937458038|\n",
      "|       3|2022-10-03|    1|  94.0999984741211|\n",
      "|       5|2022-10-03|    1| 154.8699951171875|\n",
      "+--------+----------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_metrics  = unified_model.groupBy(unified_model.store_id, unified_model.timestamp).agg(sqlf.count(unified_model.timestamp).alias('count'), sqlf.sum(unified_model.amount).alias('total'))\n",
    "store_metrics.sort('timestamp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count and sum amount transactions for each city for day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----+------------------+\n",
      "|          address| timestamp|count|             total|\n",
      "+-----------------+----------+-----+------------------+\n",
      "|        Erikville|2022-10-02|    1|    8222.150390625|\n",
      "|             null|2022-10-02|   32|254683.73966097832|\n",
      "|     Brownchester|2022-10-02|    1|382.70001220703125|\n",
      "|        Smithberg|2022-10-02|    1| 6.599999904632568|\n",
      "|       Robertland|2022-10-02|    1|       49262.71875|\n",
      "|Port Benjaminfurt|2022-10-02|    1|    65182.94921875|\n",
      "|         Lanefurt|2022-10-02|    2|            197.25|\n",
      "|       Grahamstad|2022-10-02|    1|    52058.44921875|\n",
      "|         Clayview|2022-10-02|    1|  8151.77978515625|\n",
      "|       Averymouth|2022-10-02|    2| 2983.210006713867|\n",
      "|    Michelleburgh|2022-10-02|    2| 65990.03750038147|\n",
      "|        Hicksview|2022-10-03|    2|  4112.47998046875|\n",
      "|             null|2022-10-03|   53| 716727.3829708099|\n",
      "|     Edwardsburgh|2022-10-03|    1|  9.40999984741211|\n",
      "|      Melissafurt|2022-10-03|    5|  54707.2892036438|\n",
      "|        Erikville|2022-10-03|    3|16104.099482536316|\n",
      "|       Grahamstad|2022-10-03|    1|0.6299999952316284|\n",
      "|         Lanefurt|2022-10-03|    1| 757.1300048828125|\n",
      "|         Clayview|2022-10-03|    2| 86.92000198364258|\n",
      "|      Lake Karina|2022-10-03|    3|19241.859909057617|\n",
      "|     Brownchester|2022-10-03|    2|  634.739990234375|\n",
      "|Port Benjaminfurt|2022-10-03|    3| 75357.25785064697|\n",
      "|New Marthaborough|2022-10-03|    1|27.510000228881836|\n",
      "|        Smithberg|2022-10-03|    2|  66015.8916015625|\n",
      "|       Robertland|2022-10-03|    2|    19467.05078125|\n",
      "|    Port Loriview|2022-10-03|    1| 79.18000030517578|\n",
      "|     West Nichole|2022-10-03|    4| 61547.32928466797|\n",
      "|      Veronicaton|2022-10-03|    4| 46174.09814453125|\n",
      "|  Lake Jamesville|2022-10-04|    1|  2626.56005859375|\n",
      "|        Smithberg|2022-10-04|    3| 8.729999959468842|\n",
      "|Port Benjaminfurt|2022-10-04|    2| 16567.86962890625|\n",
      "|       Averymouth|2022-10-04|    3|  16624.2197265625|\n",
      "|New Marthaborough|2022-10-04|    1| 425.2799987792969|\n",
      "|      Veronicaton|2022-10-04|    2| 59.49000072479248|\n",
      "|         Allenton|2022-10-04|    2| 7872.330116271973|\n",
      "|         Clayview|2022-10-04|    2|3527.7000122070312|\n",
      "|       Robertland|2022-10-04|    1|   7341.8701171875|\n",
      "|      Lake Karina|2022-10-04|    4|140715.79671478271|\n",
      "|     Brownchester|2022-10-04|    2| 19.72000026702881|\n",
      "|    Port Loriview|2022-10-04|    1| 73.33000183105469|\n",
      "|     West Nichole|2022-10-04|    1|    56969.12890625|\n",
      "|        Erikville|2022-10-04|    1| 6.329999923706055|\n",
      "|       Grahamstad|2022-10-04|    1| 71.47000122070312|\n",
      "|             null|2022-10-04|   82| 908732.3669177517|\n",
      "|    Michelleburgh|2022-10-04|    2|53306.920471191406|\n",
      "|      Melissafurt|2022-10-04|    1|     22232.9296875|\n",
      "|         Lanefurt|2022-10-04|    2|    28880.05078125|\n",
      "|             null|2022-10-05|   33|  325503.684027642|\n",
      "|     Edwardsburgh|2022-10-05|    1| 613.1500244140625|\n",
      "|       Averymouth|2022-10-05|    1| 207.1300048828125|\n",
      "|New Marthaborough|2022-10-05|    1|    35858.26953125|\n",
      "|      Veronicaton|2022-10-05|    1|            375.75|\n",
      "|        Smithberg|2022-10-05|    1| 566.0599975585938|\n",
      "|         Allenton|2022-10-05|    1|     9997.73046875|\n",
      "|        Hicksview|2022-10-05|    1| 91.77999877929688|\n",
      "|  Lake Jamesville|2022-10-05|    1| 583.0599975585938|\n",
      "|Port Benjaminfurt|2022-10-05|    3| 81730.82953119278|\n",
      "|         Clayview|2022-10-05|    1|  4171.10986328125|\n",
      "|    Port Loriview|2022-10-05|    1|   27460.400390625|\n",
      "|     West Nichole|2022-10-05|    1| 9.289999961853027|\n",
      "|     Brownchester|2022-10-05|    2| 49035.09078979492|\n",
      "|        Erikville|2022-10-05|    2| 136.1299991607666|\n",
      "+-----------------+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_transactions = unified_model.groupBy(unified_model.address, unified_model.timestamp).agg(sqlf.count('*').alias('count'), sqlf.sum(unified_model.amount).alias('total'))\n",
    "cities_transactions.sort('timestamp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKFILLING ALL THE NULL VALUES FROM THE COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from faker import Faker\n",
    "\n",
    "def backfillStoreID(x) -> int:\n",
    "    if x is None:\n",
    "        return random.randint(1,20)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def backfillPhoneNumber(x) -> int:\n",
    "    fake = Faker()\n",
    "    if x is None:\n",
    "        return fake.msisdn()[3:]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def backfillAddress(x) -> str:\n",
    "    if x is None:\n",
    "        return random.choice(['Allenton', 'Hicksview', 'Smithberg', 'Robertland', 'Veronicaton', 'Lake Jamesville', 'Port Benjaminfurt', 'Averymouth', 'Erikville', 'Port Loriview', 'Grahamstad', 'Edwardsburgh', 'New Marthaborough', 'Melissafurt', 'Lanefurt', 'Clayview', 'West Nichole', 'Brownchester', 'Lake Karina', 'Michelleburgh'])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def backfillCustomerID(x) -> int:\n",
    "    if x is None:\n",
    "        return random.randint(1, 900)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "backfillStoreId_udf = sqlf.udf(lambda x : backfillStoreID(x), IntegerType())\n",
    "backfillPhoneNumber_udf = sqlf.udf(lambda x: backfillPhoneNumber(x), StringType())\n",
    "backfillAddress_udf = sqlf.udf(lambda x: backfillAddress(x), StringType())\n",
    "backfillCustomerID_udf = sqlf.udf(lambda x: backfillCustomerID(x), LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_2 = unified_model\n",
    "testing_2.repartition(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_2 = testing_2.withColumn('new_type', sqlf.when(testing_2.type.isNull(), sqlf.rand()).otherwise(testing_2.type))\n",
    "testing_2 = testing_2.withColumn('b_type', sqlf.when(testing_2.new_type >= 0.5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Store ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_2 = testing_2.withColumn('b_store_id', backfillStoreId_udf(testing_2.store_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_2 = testing_2.withColumn('b_phone_number', backfillPhoneNumber_udf(testing_2.phone_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_2 = testing_2.withColumn('b_address', backfillAddress_udf(testing_2.address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Customer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_2 = testing_2.withColumn('b_customer_id', backfillCustomerID_udf(testing_2.customer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "backfilled_DF = testing_2.select('id', 'b_customer_id', 'customer_id', 'store_id', 'b_store_id', 'type', 'b_type', 'customer_first_name', 'customer_last_name', 'b_phone_number', 'phone_number', 'b_address', 'address', 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9719:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+--------+----------+----+------+-------------------+------------------+--------------+------------+-----------------+-----------------+----------+\n",
      "| id|b_customer_id|customer_id|store_id|b_store_id|type|b_type|customer_first_name|customer_last_name|b_phone_number|phone_number|        b_address|          address| timestamp|\n",
      "+---+-------------+-----------+--------+----------+----+------+-------------------+------------------+--------------+------------+-----------------+-----------------+----------+\n",
      "|  2|          327|       null|    null|         5|   1|     1|             Brenda|           Sanchez|    0283172417|        null|     Brownchester|             null|2022-10-02|\n",
      "|  4|          502|       null|    null|         7|   0|     0|               Dawn|              Hill|    4808411130|        null|         Allenton|             null|2022-10-03|\n",
      "|  6|          857|       null|    null|         2|   0|     0|            Jeffrey|          Gonzales|    5566309128|        null|        Hicksview|             null|2022-10-04|\n",
      "|  7|          544|       null|      17|        17|null|     0|             Thomas|            Oliver|    7493862133|        null|     Brownchester|             null|2022-10-05|\n",
      "| 10|          335|       null|      20|        20|null|     1|            William|             Reyes|    6685617783|        null|         Lanefurt|             null|2022-10-04|\n",
      "| 11|          247|       null|       2|         2|null|     0|             Jeremy|             Moody|    0999293064|        null|    Port Loriview|             null|2022-10-03|\n",
      "| 12|           12|         12|    null|        18|null|     0|           Patricia|          Williams|    1624390971|  1624390971|Port Benjaminfurt|Port Benjaminfurt|2022-10-03|\n",
      "| 17|           17|         17|    null|        18|null|     1|          Stephanie|            Chavez|    9952997214|  9952997214|    Michelleburgh|    Michelleburgh|2022-10-02|\n",
      "| 18|           18|         18|    null|        16|null|     0|             Denise|          Anderson|    5718166161|  5718166161|      Melissafurt|      Melissafurt|2022-10-03|\n",
      "| 19|          848|       null|    null|        10|   0|     0|              Sarah|             Craig|    8296937024|        null|         Allenton|             null|2022-10-04|\n",
      "| 23|           95|       null|      18|        18|null|     1|             Brandi|             Ortiz|    2710423413|        null|      Lake Karina|             null|2022-10-03|\n",
      "| 28|          785|       null|      13|        13|null|     0|            Anthony|            Harper|    0592964725|        null|       Averymouth|             null|2022-10-03|\n",
      "| 33|           33|         33|    null|         8|null|     0|               John|             Burns|    8671875822|  8671875822|Port Benjaminfurt|Port Benjaminfurt|2022-10-03|\n",
      "| 41|           41|         41|    null|        16|null|     0|           Veronica|          Williams|    4852270998|  4852270998|         Lanefurt|         Lanefurt|2022-10-02|\n",
      "| 43|           43|         43|    null|         5|null|     0|            Zachary|            Ramsey|    6457314142|  6457314142|        Hicksview|        Hicksview|2022-10-05|\n",
      "| 45|          136|       null|      17|        17|null|     0|             Joshua|           Johnson|    1341212628|        null|New Marthaborough|             null|2022-10-02|\n",
      "| 46|          808|       null|    null|        11|   0|     0|              Sarah|             Jones|    6852926745|        null|New Marthaborough|             null|2022-10-04|\n",
      "| 47|          823|       null|    null|         4|   0|     0|              Peter|              Mack|    1847969588|        null|     Edwardsburgh|             null|2022-10-04|\n",
      "| 54|           54|         54|    null|        18|null|     1|            Michael|           Johnson|    8947261122|  8947261122|    Port Loriview|    Port Loriview|2022-10-03|\n",
      "| 57|          460|       null|       9|         9|null|     0|            Matthew|             Scott|    9231398396|        null|     West Nichole|             null|2022-10-04|\n",
      "+---+-------------+-----------+--------+----------+----+------+-------------------+------------------+--------------+------------+-----------------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "backfilled_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  type  store_id      amount customer_first_name customer_last_name  \\\n",
      "0    386   NaN      11.0       $5.04              Colton             Reeves   \n",
      "1    417   NaN       6.0     $583.96               Scott            Roberts   \n",
      "2    624   0.0       NaN     $206.94             Kristen             Porter   \n",
      "3    324   NaN      12.0     $459.10             Michael              Gross   \n",
      "4     47   1.0       NaN     $488.94              Sergio             Wilson   \n",
      "..   ...   ...       ...         ...                 ...                ...   \n",
      "195  166   1.0       NaN       $5.66               Karla             Duarte   \n",
      "196  255   0.0       NaN   $3,428.23             Kenneth           Gonzalez   \n",
      "197  125   1.0       NaN   $3,579.21             Timothy               Cruz   \n",
      "198  692   NaN      16.0   $1,072.52            Brittany          Velazquez   \n",
      "199  685   NaN      19.0  $34,941.97                Dawn              Smith   \n",
      "\n",
      "               timestamp  \n",
      "0    2022-10-18T04:05:07  \n",
      "1    2022-10-17T17:01:54  \n",
      "2    2022-10-18T21:56:29  \n",
      "3    2022-10-19T03:39:18  \n",
      "4    2022-10-17T07:01:51  \n",
      "..                   ...  \n",
      "195  2022-10-18T06:52:35  \n",
      "196  2022-10-18T13:17:16  \n",
      "197  2022-10-18T11:16:08  \n",
      "198  2022-10-18T13:56:56  \n",
      "199  2022-10-18T14:23:36  \n",
      "\n",
      "[200 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('/Users/gonzo/Desktop/capstone_project/airflow/dags/storage/rdbms_temp/')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('CapstoneENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "874b0dd84e8b98467c3d8042fd47d9e77e6c7558a12d29c4392edcac270f293e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
