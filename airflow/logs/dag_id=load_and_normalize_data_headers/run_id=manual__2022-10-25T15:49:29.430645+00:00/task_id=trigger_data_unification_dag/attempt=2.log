[2022-10-25T15:56:44.851+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: load_and_normalize_data_headers.trigger_data_unification_dag manual__2022-10-25T15:49:29.430645+00:00 [queued]>
[2022-10-25T15:56:44.879+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: load_and_normalize_data_headers.trigger_data_unification_dag manual__2022-10-25T15:49:29.430645+00:00 [queued]>
[2022-10-25T15:56:44.881+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-25T15:56:44.883+0000] {taskinstance.py:1363} INFO - Starting attempt 2 of 2
[2022-10-25T15:56:44.885+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-25T15:56:44.912+0000] {taskinstance.py:1383} INFO - Executing <Task(TriggerDagRunOperator): trigger_data_unification_dag> on 2022-10-25 15:49:29.430645+00:00
[2022-10-25T15:56:44.933+0000] {standard_task_runner.py:54} INFO - Started process 5281 to run task
[2022-10-25T15:56:44.951+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'load_and_normalize_data_headers', 'trigger_data_unification_dag', 'manual__2022-10-25T15:49:29.430645+00:00', '--job-id', '602', '--raw', '--subdir', 'DAGS_FOLDER/pyspark_jobs.py', '--cfg-path', '/tmp/tmpglujssmz']
[2022-10-25T15:56:44.958+0000] {standard_task_runner.py:83} INFO - Job 602: Subtask trigger_data_unification_dag
[2022-10-25T15:56:44.962+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/pyspark_jobs.py
[2022-10-25T15:56:45.134+0000] {task_command.py:384} INFO - Running <TaskInstance: load_and_normalize_data_headers.trigger_data_unification_dag manual__2022-10-25T15:49:29.430645+00:00 [running]> on host faf467258cbc
[2022-10-25T15:56:45.257+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=load_and_normalize_data_headers
AIRFLOW_CTX_TASK_ID=trigger_data_unification_dag
AIRFLOW_CTX_EXECUTION_DATE=2022-10-25T15:49:29.430645+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-10-25T15:49:29.430645+00:00
[2022-10-25T15:56:45.314+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/trigger_dagrun.py", line 141, in execute
    replace_microseconds=False,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/trigger_dag.py", line 130, in trigger_dag
    replace_microseconds=replace_microseconds,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/trigger_dag.py", line 52, in _trigger_dag
    raise DagNotFound(f"Dag id {dag_id} not found")
airflow.exceptions.DagNotFound: Dag id unify_datasets not found
[2022-10-25T15:56:45.326+0000] {taskinstance.py:1406} INFO - Marking task as FAILED. dag_id=load_and_normalize_data_headers, task_id=trigger_data_unification_dag, execution_date=20221025T154929, start_date=20221025T155644, end_date=20221025T155645
[2022-10-25T15:56:45.349+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 602 for task trigger_data_unification_dag (Dag id unify_datasets not found; 5281)
[2022-10-25T15:56:45.402+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-25T15:56:45.443+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
