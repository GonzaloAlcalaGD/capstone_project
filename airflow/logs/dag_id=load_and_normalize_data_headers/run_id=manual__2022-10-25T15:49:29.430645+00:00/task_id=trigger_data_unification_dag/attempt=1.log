[2022-10-25T15:54:41.368+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: load_and_normalize_data_headers.trigger_data_unification_dag manual__2022-10-25T15:49:29.430645+00:00 [queued]>
[2022-10-25T15:54:41.397+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: load_and_normalize_data_headers.trigger_data_unification_dag manual__2022-10-25T15:49:29.430645+00:00 [queued]>
[2022-10-25T15:54:41.399+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-25T15:54:41.401+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 2
[2022-10-25T15:54:41.402+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-25T15:54:41.435+0000] {taskinstance.py:1383} INFO - Executing <Task(TriggerDagRunOperator): trigger_data_unification_dag> on 2022-10-25 15:49:29.430645+00:00
[2022-10-25T15:54:41.448+0000] {standard_task_runner.py:54} INFO - Started process 5135 to run task
[2022-10-25T15:54:41.457+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'load_and_normalize_data_headers', 'trigger_data_unification_dag', 'manual__2022-10-25T15:49:29.430645+00:00', '--job-id', '601', '--raw', '--subdir', 'DAGS_FOLDER/pyspark_jobs.py', '--cfg-path', '/tmp/tmpo4zwq7te']
[2022-10-25T15:54:41.463+0000] {standard_task_runner.py:83} INFO - Job 601: Subtask trigger_data_unification_dag
[2022-10-25T15:54:41.466+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/pyspark_jobs.py
[2022-10-25T15:54:41.631+0000] {task_command.py:384} INFO - Running <TaskInstance: load_and_normalize_data_headers.trigger_data_unification_dag manual__2022-10-25T15:49:29.430645+00:00 [running]> on host faf467258cbc
[2022-10-25T15:54:41.777+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=load_and_normalize_data_headers
AIRFLOW_CTX_TASK_ID=trigger_data_unification_dag
AIRFLOW_CTX_EXECUTION_DATE=2022-10-25T15:49:29.430645+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-10-25T15:49:29.430645+00:00
[2022-10-25T15:54:41.838+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/trigger_dagrun.py", line 141, in execute
    replace_microseconds=False,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/trigger_dag.py", line 130, in trigger_dag
    replace_microseconds=replace_microseconds,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/trigger_dag.py", line 52, in _trigger_dag
    raise DagNotFound(f"Dag id {dag_id} not found")
airflow.exceptions.DagNotFound: Dag id unify_datasets not found
[2022-10-25T15:54:41.847+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=load_and_normalize_data_headers, task_id=trigger_data_unification_dag, execution_date=20221025T154929, start_date=20221025T155441, end_date=20221025T155441
[2022-10-25T15:54:41.879+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 601 for task trigger_data_unification_dag (Dag id unify_datasets not found; 5135)
[2022-10-25T15:54:41.910+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-25T15:54:41.948+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
